{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chemometrics course - 2. Unsupervised methods: Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering groups data in some way, we need to define how it does so\n",
    "unlabbeled data\n",
    "manhattan distance along axis\n",
    "euclidean distance - shortest as possible\n",
    "chebyshev distance - longest axis\n",
    "\n",
    "linear distance not always so good for high dimensionality, use cosine similarity or mahalanobis sitance based on variance\n",
    "\n",
    "depending on which model you use, you can get different separation of data\n",
    "\n",
    "heirachical clustering - divisive or agglomerative - starting from one cluster and dividing or from many and grouping\n",
    "\n",
    "k-means\n",
    "cost function - usually sum of squared means\n",
    "choose number of clusters, initialise two random centers, assign each point to a cluster, move centers to the mean senter, then repeat until convergence\n",
    "\n",
    "global or local optimum - does the initial guess choose the right area, what if it convreges wrong\n",
    "\n",
    "can solve by initialising multiple times or start with extra centers and kill the ones that have few points or us k-means++ pick centers further away from current centers\n",
    "\n",
    "how many clusters - elbow method start with less add more and then analyse cost function - where is the change. you still have to decide\n",
    "\n",
    "fast, easy, effective but - problem with global vs. local, different analysis each time, very sensitive to noise and anoutliers, difficult to use with poorly separated clusters\n",
    "\n",
    "in heirachical methods we can have groups inside of groups - displayed in dendrogram\n",
    "teach each sample as a separate cluster and join the samples, treat joined as one and find other similar samples. height can also have meaning \n",
    "to join clusters you have to choose linkage - nearest neighbours single linkage or furthest neighbours complete linkage, or average linkage\n",
    "easier to pick out outliers, will give the same result every time given same linkers, gives qualatitative assessment but more demainding\n",
    "\n",
    "divisive clustering is the same but reverse\n",
    "\n",
    "quality of clustering - not known but can be assessed by cost function \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
